# CITS5507 Lab Report 5 - 9 

*Author*: Joo Kai Tay (22489437)

## Lab 5: Networking

### Section 1: Configure inbound IP on VM
1. Configure the network adapted in VirtualBox Manager using the rule: host IP 127.0.0.1 and host port 2222 mapped to Guest Port 22

<br></br>
![Configure adapter](lab5/Screenshot%202023-09-16%20154651.png)

2. Install tasksel and openssh-server
	-  tasksel is a Debian and Ubuntu Linux command-line tool that simplifies the installation and management of groups of related packages, known as "tasks." 
	- OpenSSH is a widely used tool for securely accessing and managing remote servers via the SSH (Secure Shell) protocol.
	
<br></br>
![tasksel](lab5/Screenshot%202023-09-16%20154811.png)
<br></br>
![openssh-server](lab5/Screenshot%202023-09-16%20154847.png)

3. Starting the ssh service on the ubuntu VM
	- sudo service ssh start is used to start the SSH (Secure Shell) server on a Linux system. 

<br></br>
![ubuntu ssh](lab5/Screenshot%202023-09-16%20155101.png)

4. SSH into the Ubuntu VM from the hostOS using Putty:
<br></br>
![putty ssh](lab5/Screenshot%202023-09-16%20155143.png)
<br></br>
![putty ssh](lab5/Screenshot%202023-09-16%20155218.png)

5. Terminate the SSH service:
<br></br>
![terminate](lab5/Screenshot%202023-09-16%20155314.png)

### Section 2: Setting up an Application Load Balancer

1. The following function is used to create 2 EC2 instances in two different availability zones of ap-southeast-1. The reason ap-southeast-1 was used instead of ap-southeast-2 was due to the limit in VPCUs on ap-southeast-2 which did not allow for any new EC2 instances to be created on the region at the time of attempting this lab. 
	- It creates a security group with a name 22489437-sg.
	- It authorizes incoming SSH (Secure Shell) traffic on port 22 for the security group.
	- It generates a key pair 
	- It defines a list of availability zones (availability_zones) where EC2 instances will be launched.
	- For each availability zone, it creates an EC2 instance with specific parameters.

```
def launch_ec2_instances():
	# Create a security group
	response = ec2.create_security_group(
		GroupName=f"{student_number}-sg",
		Description="security group for development environment"
	)
	security_group_id = response['GroupId']

	# Authorize inbound SSH traffic for the security group
	ec2.authorize_security_group_ingress(
		GroupId=security_group_id,
		IpProtocol="tcp",
		FromPort=22,
		ToPort=22,
		CidrIp="0.0.0.0/0"
	)
	
	# Create a key pair and save the private key to a file
	response = ec2.create_key_pair(KeyName=f"{student_number}-key")
	private_key = response['KeyMaterial']
	private_key_file = f"{student_number}-key.pem"
	
	# Allow writing to the private key file
	os.chmod(private_key_file, 0o666)
	with open(private_key_file, 'w') as key_file:
		key_file.write(private_key)
	# Set the correct permissions for the private key file
	os.chmod(private_key_file, 0o400)
	# Copy the private key file to ~/.ssh directory
	ssh_directory = os.path.expanduser("~/.ssh")
	if not os.path.exists(ssh_directory):
		os.makedirs(ssh_directory)

	shutil.copy(private_key_file, ssh_directory)

	availability_zones = ["ap-southeast-1a", "ap-southeast-1b"]

	for i, az in enumerate(availability_zones):
		instance_name = f"{student_number}-{az}"
		
		instance_params = {
		    'ImageId': 'ami-0df7a207adb9748c7',  
		    'InstanceType': 't2.micro',  
		    'KeyName': f"{student_number}-key",
		    'SecurityGroupIds' : [security_group_id], 
		    'MinCount': 1,
		    'MaxCount': 1,
		    'Placement': {'AvailabilityZone': az},
		    'TagSpecifications': [
		        {
		            'ResourceType': 'instance',
		            'Tags': [{'Key': 'Name', 'Value': instance_name}]
		        }
		    ]
		}

		# Launch an EC2 instance
		response = ec2.run_instances(**instance_params)

		instance_id = response['Instances'][0]['InstanceId']

		# Wait for the instance to be up and running
		ec2.get_waiter('instance_running').wait(InstanceIds=[instance_id])

		# Describe the instance to get its public IP address
		response = ec2.describe_instances(InstanceIds=[instance_id])
		public_ip_address = response['Reservations'][0]['Instances'][0]['PublicIpAddress']

		print(f"Instance {i+1} created successfully in Availability Zone {az} with Public IP: {public_ip_address}")
```
The created EC2 instances can be observed below. Note that the highlighted public IP addresses and availability zones in the AWS console correspond to the terminal output. 
<br></br>
![EC2](lab5/Screenshot%202023-09-17%20105046.png)
<br></br>
![EC2](lab5/Screenshot%202023-09-17%20105108.png)
<br></br>
![EC2](lab5/Screenshot%202023-09-17%20105138.png)
<br></br>
![EC2](lab5/Screenshot%202023-09-17%20105206.png)

2. The code below creates an application load balancer. 
    - The code creates the load balancer and specifies the two region subnets retreived from step 1. 
    - The code creates a listener with a default rule Protocol: HTTP and Port 80 forwarding on to the target group
    - The code creates a target group using the VPC from step 1
    - The code registers the two EC2 instances from step 1 as targets
```
def create_load_balancer():
	vpc_id = 'vpc-02806703abdc316d0'
	security_group_id = 'sg-0021774194b407020'
	subnet_ids = ['subnet-080783bde78702ba9', 'subnet-0da033b36a320696f']
	
	response = elb.create_load_balancer(
		Name='22489437-LoadBalancer',
		Subnets=subnet_ids,
		SecurityGroups=[security_group_id],
		Scheme='internet-facing',  
		Tags=[
		    {
		        'Key': 'Name',
		        'Value': '22489437-LoadBalancer'
		    },
		]
	) 
	
	load_balancer_arn = response['LoadBalancers'][0]['LoadBalancerArn']
	print(f"Load Balancer ARN: {load_balancer_arn}")

	# Create a target group
	response = elb.create_target_group(
		Name='22489437-target-group',
		Protocol='HTTP',
        Port=80,
        VpcId=vpc_id,
        TargetType='instance'
    )
	
    # Get the ARN of the target group
	target_group_arn = response['TargetGroups'][0]['TargetGroupArn']
	print(f"Target Group ARN: {target_group_arn}")

	# Create a listener for HTTP traffic (Port 80)
	response = elb.create_listener(
        DefaultActions=[
            {
                'Type': 'forward',
                'TargetGroupArn': target_group_arn,
            },
        ],
        LoadBalancerArn=load_balancer_arn,
        Port=80,
        Protocol='HTTP',
    )

	listener_arn = response['Listeners'][0]['ListenerArn']
	print(f"Listener ARN: {listener_arn}")
	
	instance_1_id = 'i-01624737c61ac9b4d'
	instance_2_id = 'i-0e105acc6d5603f70'

	# Register the instances in the target group
	elb.register_targets(
		TargetGroupArn=target_group_arn,
		Targets=[
		    {'Id': instance_1_id},
		    {'Id': instance_2_id},
		]
	)

	# Print registration status
	print("Targets registered successfully.")
```

The following screenshots show the output of running the code as well as the results in the AWS terminal.

<br></br>
![LB](lab5/Screenshot%202023-09-17%20113549.png)
<br></br>
![LB](lab5/Screenshot%202023-09-17%20114706.png)
<br></br>
![LB](lab5/Screenshot%202023-09-17%20114720.png)
<br></br>
![LB](lab5/Screenshot%202023-09-17%20114733.png)
<br></br>
![LB](lab5/Screenshot%202023-09-17%20114802.png)

3. In this step, we will SSH into each of the instances created in step 1 and install Apache2. Screenshots showing this process for one of the EC2 instances have been attached:
	- Apache2 is used to serve web content, including web pages, images, videos, and other files, to clients (typically web browsers) that request them over the internet. 

<br></br>
![LB](lab5/Screenshot%202023-09-17%20115049.png)
<br></br>
![LB](lab5/Screenshot%202023-09-17%20115143.png)
<br></br>
![LB](lab5/Screenshot%202023-09-17%20115721.png)

4. In this step we will edit the `/var/www/html/index.html` file to report the instance name and availability zone. 
<br></br>
![VI](lab5/Screenshot%202023-09-17%20124033.png)
<br></br>
![VI](lab5/Screenshot%202023-09-17%20124338.png)

5. By refreshing the page repeatedly, we can access both EC2 instances
<br></br>
![ec2](lab5/Screenshot%202023-09-17%20124351.png)
<br></br>
![ec2](lab5/Screenshot%202023-09-17%20124359.png)

## Lab 6: Networking

### Section 1: Create an EC2 Instance

1. The code below was used to create an EC2 instance on ap-southeast-2c. For the lab this week, there was available capacity on ap-southeast-2 so there was no need to create the instance on another region. The AMI provided in lab 2 `ami-d38a4ab1` had a heavily outdated version of python and other utilities that are not compatible with modern programs, therefore an updated AMI was selected instead `ami-0310483fb2b488153`.
	- It creates a security group with a name 22489437-sg.
	- It authorizes incoming SSH (Secure Shell) traffic on port 22 for the security group.
	- It generates a key pair 
	- It defines a list of availability zones (availability_zones) where EC2 instances will be launched.
	- For each availability zone, it creates an EC2 instance with specific parameters.

![ec2](lab6/Screenshot%202023-09-23%20084621.png)

```
def launch_ec2_instances():
	# Create a security group
	response = ec2.create_security_group(
		GroupName=f"{student_number}-sg",
		Description="security group for development environment"
	)
	security_group_id = response['GroupId']

	# Authorize inbound SSH traffic for the security group
	ec2.authorize_security_group_ingress(
		GroupId=security_group_id,
		IpProtocol="tcp",
		FromPort=22,
		ToPort=22,
		CidrIp="0.0.0.0/0"
	)
	
	ec2.authorize_security_group_ingress(
		GroupId=security_group_id,
		IpProtocol="tcp",
		FromPort=80,
		ToPort=80,
		CidrIp="0.0.0.0/0"
	)
	
	# Create a key pair and save the private key to a file
	response = ec2.create_key_pair(KeyName=f"{student_number}-key")
	private_key = response['KeyMaterial']
	private_key_file = f"{student_number}-key.pem"
	
	# Allow writing to the private key file
	os.chmod(private_key_file, 0o666)
	with open(private_key_file, 'w') as key_file:
		key_file.write(private_key)
	# Set the correct permissions for the private key file
	os.chmod(private_key_file, 0o400)
	# Copy the private key file to ~/.ssh directory
	ssh_directory = os.path.expanduser("~/.ssh")
	if not os.path.exists(ssh_directory):
		os.makedirs(ssh_directory)

	shutil.copy(private_key_file, ssh_directory)

	availability_zones = ["ap-southeast-2b", "ap-southeast-2c"]

	for i, az in enumerate(availability_zones):
		instance_name = f"{student_number}-{az}"
		
		instance_params = {
		    'ImageId': 'ami-0310483fb2b488153',  
		    'InstanceType': 't2.micro',  
		    'KeyName': f"{student_number}-key",
		    'SecurityGroupIds' : [security_group_id], 
		    'MinCount': 1,
		    'MaxCount': 1,
		    'Placement': {'AvailabilityZone': az},
		    'TagSpecifications': [
		        {
		            'ResourceType': 'instance',
		            'Tags': [{'Key': 'Name', 'Value': instance_name}]
		        }
		    ]
		}

		# Launch an EC2 instance
		response = ec2.run_instances(**instance_params)

		instance_id = response['Instances'][0]['InstanceId']

		# Wait for the instance to be up and running
		ec2.get_waiter('instance_running').wait(InstanceIds=[instance_id])

		# Describe the instance to get its public IP address
		response = ec2.describe_instances(InstanceIds=[instance_id])
		public_ip_address = response['Reservations'][0]['Instances'][0]['PublicIpAddress']

		print(f"Instance {i+1} created successfully in Availability Zone {az} with Public IP: {public_ip_address}")
```

![ec2](lab6/Screenshot%202023-09-23%20084642.png)

2. Using the private key obtained and public IP address obtained from step 1, SSH into the EC2 instance and install the Python 3 virtual environment package.
	- venv stands for "virtual environment" in Python. It is a module in the Python standard library (available in Python 3.3 and later) that allows you to create isolated environments for Python projects. Virtual environments are useful for managing project-specific dependencies and avoiding conflicts between different Python projects that may require different packages or versions of packages.

![ec2](lab6/Screenshot%202023-09-23%20084735.png)

![ec2](lab6/Screenshot%202023-09-23%20085139.png)

![ec2](lab6/Screenshot%202023-09-23%20085220.png)

![ec2](lab6/Screenshot%202023-09-23%20085949.png)

3. Creating a directory with path `/opt/wwc/mysites` and setting up the virtual environment.

![ec2](lab6/Screenshot%202023-09-23%20090113.png)

![ec2](lab6/Screenshot%202023-09-23%20090202.png)

4. A Django project is a collection of configurations and apps for a particular website. In this step we install Django and create a new Django app named polls.
	- `lab`: The configuration directory
	- `polls`: The directory containing the app
	- `manage.py`: The command line utility that lets us interact with the new app

![ec2](lab6/Screenshot%202023-09-23%20090235.png)

![ec2](lab6/Screenshot%202023-09-23%20090309.png)

### Section 2: Install and Congigure Nginx

1. Installing and configuring nginx:
	- Nginx is a popular open-source web server software that can also be used as a reverse proxy, load balancer, mail proxy, and HTTP cache. 
	- The configuration file is edited to tell Nginx to pass requests to the backend server running on the same machine 127.0.0.1 at port 8000.

![ec2](lab6/Screenshot%202023-09-23%20090326.png)

![ec2](lab6/Screenshot%202023-09-23%20090638.png)

![ec2](lab6/Screenshot%202023-09-23%20090700.png)

2. Restarting Nginx so that the changes from step 1 take effect:

![ec2](lab6/Screenshot%202023-09-23%20090723.png)

3. Using the command `python3 manage.py runserver 8000` to start Django’s development web server at port 8000.

![ec2](lab6/Screenshot%202023-09-23%20090754.png)

4. Trying the access the public IP address of the EC2 instance results in an error:

![ec2](lab6/Screenshot%202023-09-23%20090856.png)

![ec2](lab6/Screenshot%202023-09-23%20161507.png)

### Section 3: Change the code

1. Editing `polls/views.py`
	- This code creates a simple view that returns an HTTP response with the text "Hello, world." when it's called.

![ec2](lab6/Screenshot%202023-09-23%20093018.png)

2. Edit `polls/urls.py`
	- This code defines a URL pattern for this view in the urls.py file, so that Django knows which view to call for a given URL.

![ec2](lab6/Screenshot%202023-09-23%20093046.png)

3. Edit `lab/urls.py`
	- The code configures the URL patterns for the Django project. 

![ec2](lab6/Screenshot%202023-09-23%20093139.png)

4. Running the application and getting `Hello, world`

![ec2](lab6/Screenshot%202023-09-23%20093201.png)

![ec2](lab6/Screenshot%202023-09-23%20093451.png)

### Section 4: Adding an application load balance (ALB)

1. The code below creates an application load balancer, specifies the region subnet where the EC2 instance resides, creates a listener with a default rule Protocol: HTTP and Port 80 forwarding.

```
def create_load_balancer():
	vpc_id = 'vpc-00da1b229d10a51b6'
	security_group_id = 'sg-0fb8992bd2473b7bc'
	subnet_ids = ['subnet-0c1878c6a739707b7', 'subnet-0102e73cd4ff52b52']
	
	response = elb.create_load_balancer(
		Name='22489437-LoadBalancer',
		Subnets=subnet_ids,
		SecurityGroups=[security_group_id],
		Scheme='internet-facing',  
		Tags=[
		    {
		        'Key': 'Name',
		        'Value': '22489437-LoadBalancer'
		    },
		]
	) 
	
	load_balancer_arn = response['LoadBalancers'][0]['LoadBalancerArn']
	print(f"Load Balancer ARN: {load_balancer_arn}")

	# Create a target group
	response = elb.create_target_group(
		Name='22489437-target-group',
		Protocol='HTTP',
        Port=80,
        VpcId=vpc_id,
        HealthCheckProtocol='HTTP',
        HealthCheckPort='80',
        HealthCheckPath='/polls/',
        HealthCheckIntervalSeconds=30,
        HealthCheckTimeoutSeconds=5,
        HealthyThresholdCount=5,
        UnhealthyThresholdCount=2,
        Matcher={
            'HttpCode': '200'
        },
        TargetType='instance'
    )
	
    # Get the ARN of the target group
	target_group_arn = response['TargetGroups'][0]['TargetGroupArn']
	print(f"Target Group ARN: {target_group_arn}")

	# Create a listener for HTTP traffic (Port 80)
	response = elb.create_listener(
        DefaultActions=[
            {
                'Type': 'forward',
                'TargetGroupArn': target_group_arn,
            },
        ],
        LoadBalancerArn=load_balancer_arn,
        Port=80,
        Protocol='HTTP',
    )

	listener_arn = response['Listeners'][0]['ListenerArn']
	print(f"Listener ARN: {listener_arn}")
	
	
	instance_1_id = 'i-0164b69ac55068896'

	# Register the instances in the target group
	elb.register_targets(
		TargetGroupArn=target_group_arn,
		Targets=[
		    {'Id': instance_1_id},
		]
	)

	# Print registration status
	print("Targets registered successfully.")
```

![ec2](lab6/Screenshot%202023-09-23%20104033.png)

![ec2](lab6/Screenshot%202023-09-23%20104738.png)

2. Viewing the health check on the `/polls/` page

![ec2](lab6/Screenshot%202023-09-23%20170440.png)

3. Accessing the site using the url:

![ec2](lab6/Screenshot%202023-09-23%20105140.png)

## Lab 7: DevOps

### Step 1: Create an EC2 Instance

1. The code used in Labs 5 and 6 was used to create an EC2 instance in availability zone ap-southeast-2b

![ec2](lab7/Screenshot%202023-09-29%20133134.png)

![ec2](lab7/Screenshot%202023-09-29%20133202.png)

### Step 2: Install Fabric

1. Fabric is a Python library used to perform Linux shell commands remotely over SSH. It is a command-line tool that uses SSH for application deployment or for administration tasks. In this lab we will be using fabric to automate the deployment of a django app on the remote EC2 instance.

![ec2](lab7/Screenshot%202023-09-29%20133531.png)

The configuration file located in `~/.ssh` contains the IPV4 DNS of the EC2 instance and the path of the private key file on the local VM to allow SSH access into the EC2 instance. 

![ec2](lab7/Screenshot%202023-09-29%20133918.png)

2. Testing Fabric

![ec2](lab7/Screenshot%202023-09-29%20135739.png)

### Step 3: Python Script to automate installation of nginx

1. The following script is used to automate the instllation of nginx on the EC2 instance.

```
def create_nginx():
    with Connection('<ec2instance>') as c:
        c.sudo('apt update -y')
        c.sudo('apt upgrade -y')
        c.sudo('apt install nginx -y')
        c.sudo('rm /etc/nginx/sites-enabled/default')
        os.system('scp -i ~/.ssh/22489437-key.pem default ubuntu@ec2-13-211-104-183.ap-southeast-2.compute.amazonaws.com:/home/ubuntu')
        c.sudo('mv /home/ubuntu/default /etc/nginx/sites-enabled/')
        c.sudo('service nginx restart')
```

The script is run on the local VM:

![ec2](lab7/Screenshot%202023-09-29%20170823.png)

Checking if nginx is installed on the EC2 instance:

![ec2](lab7/Screenshot%202023-09-29%20171231.png)

### Step 4: Python script to install Django app

1. The following script is used to automate the installation of the Django app on the EC2 instanc:

```
def create_django_app():
	with Connection('<ec2instance>') as c:
		c.sudo('mkdir -p /opt/wwc/mysites')
		c.sudo('apt-get install python3-pip -y')
		c.sudo('apt install python3-django -y')
		c.sudo('pip3 install django')
		c.run('django-admin startproject lab')
		c.sudo('chmod 777 ./lab')
		c.run('cd ./lab && python3 manage.py startapp polls')
		c.sudo('mv ./lab /opt/wwc/mysites')
		c.sudo('rm /opt/wwc/mysites/lab/polls/views.py')
		c.sudo('rm /opt/wwc/mysites/lab/lab/urls.py')
		os.system('scp -i ~/.ssh/22489437-key.pem ./polls/views.py ubuntu@ec2-13-211-104-183.ap-southeast-2.compute.amazonaws.com:/opt/wwc/mysites/lab/polls')
		os.system('scp -i ~/.ssh/22489437-key.pem ./polls/urls.py ubuntu@ec2-13-211-104-183.ap-southeast-2.compute.amazonaws.com:/opt/wwc/mysites/lab/polls')
		os.system('scp -i ~/.ssh/22489437-key.pem ./lab/urls.py ubuntu@ec2-13-211-104-183.ap-southeast-2.compute.amazonaws.com:/opt/wwc/mysites/lab/lab')
		c.run('cd /opt/wwc/mysites/lab && python3 manage.py runserver 8000')
```

The script is run on the local VM:

![ec2](lab7/Screenshot%202023-09-29%20220745.png)

Checking to see if `Hello world` is displayed when visiting the Public IPv4 DNS of the EC2 instance:

![ec2](lab7/Screenshot%202023-09-29%20222351.png)

## Lab 8: AI

### Step 1: Install and run jupyter notebooks

1. Jupyter notebook was already installed on this machine for CITS5508 machine learning. Therefore, only the command to launch it was needed.

![ec2](lab8/Screenshot%202023-10-06%20145946.png)

2. This brings up Jupyter notebook in the browser where we can create a notebook for this lab named `lab8.ipynb` which is an Interactive Python Notebook. 

![ec2](lab8/Screenshot%202023-10-06%20145957.png)

### Step 2: Setup Python Environment

1. The following command is run `pip3 install sagemaker pandas ipykernel`. 
	- This installs sagemaker, which is the Python SDK for Amazon SageMaker, that allows us to build, train, and deploy machine learning models. By installing this SDK, we can interact with the SageMaker service directly from the Python environment on the virtual machine. 
	- Pandas is a popular data manipulation and analysis library for Python. 
	- Ipykernel allows the python environment to interface with Jupyrer.

### Step 3: Create Role and Bucket

1. Creating a S3 bucket to store the files generated in the later parts of the machine learning process:

![ec2](lab8/Screenshot%202023-10-06%20142411.png)

2. Creating the folders `sagemaker/22489437-hpo-xgboost-dm`

![ec2](lab8/Screenshot%202023-10-06%20142510.png)

3. Due to security reasons, students are not allowed to directly create IAM roles. Therefore, we will use the code provided in the labsheet to use the already created IAM role.
	- `smclient = boto3.Session().client("sagemaker")`: This initializes a low-level client representing Amazon SageMaker. So, smclient will be used to make requests to the SageMaker service.
	- `sagemaker_role = iam.get_role(RoleName='Role_AWS_SageMaker')['Role']['Arn']`: This uses the IAM client to retrieve the ARN of a specific IAM role named 'Role_AWS_SageMaker'. This ARN can later be used to grant permissions or interact with the role in various ways within AWS.

```
region = 'ap-southeast-2'
smclient = boto3.Session().client("sagemaker")

iam = boto3.client('iam')
sagemaker_role = iam.get_role(RoleName='Role_AWS_SageMaker')['Role']['Arn']

student_id = "STUDENTID"
bucket = 'YOUR_BUCKET_NAME_HERE'
prefix = f"sagemaker/{student_id}-hpo-xgboost-dm"
```

4. The code provided fails to execute as the role `Role_AWS_SageMaker` does not exist. Therefore, we log into the IAM system in the AWS console and find that the actual name of the created roles is `SageMakerRole`. When using the actual name of the role, the code executes and we can proceed to the next step. 

![ec2](lab8/Screenshot%202023-10-06%20150902.png)

### Step 4: Download Dataset

1. We download and extract the direct marketing dataset from UCI’s ML Repository.

![ec2](lab8/Screenshot%202023-10-06%20145442.png)

The dataset is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. 

2. We read the dataset into a Pandas dataframe and inspect the contents:
	- We see that the data fame has 41188 rows and 21 columns

```
data = pd.read_csv("./bank-additional/bank-additional-full.csv", sep=";")
pd.set_option("display.max_columns", 500)  # Make sure we can see all of the columns
pd.set_option("display.max_rows", 50)  # Keep the output on one page
data
```

![ec2](lab8/Screenshot%202023-10-06%20153527.png)

3. In order to answer which variables are categorical and numerical, the following function was used:
	- This makes use of the `select_dtypes ` method to filter out the categorical and numeric columns

```
# numeric columns
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
print("Numeric columns:", numeric_cols)

# categorical columns
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
print("Categorical columns:", categorical_cols)
```

![ec2](lab8/Screenshot%202023-10-06%20153537.png)

4. The following code performs some transformations on the data:
	- `data["no_previous_contact"] = np.where(data["pdays"] == 999, 1, 0)`: This line creates a new column in the DataFrame data named "no_previous_contact". The values in this column are determined by the values in the "pdays" column. If a value in "pdays" is 999, the new column gets a value of 1; otherwise, it gets 0. 
	- `data["not_working"] = np.where(np.in1d(data["job"], ["student", "retired", "unemployed"]), 1, 0)`: This line creates another new column named "not_working". It checks the "job" column of the DataFrame. If the job description is either "student", "retired", or "unemployed", the new column gets a value of 1, indicating the person is not actively employed. Otherwise, it gets 0.
	- `model_data = pd.get_dummies(data)`: This line creates a new DataFrame, model_data, by converting the categorical variables in data into "indicator" variables. For each level in a categorical variable, a new column will be created to indicate if it is present or not. 

```
data["no_previous_contact"] = np.where(
    data["pdays"] == 999, 1, 0
)  # Indicator variable to capture when pdays takes a value of 999
data["not_working"] = np.where(
    np.in1d(data["job"], ["student", "retired", "unemployed"]), 1, 0
)  # Indicator for individuals not actively employed
model_data = pd.get_dummies(data)  # Convert categorical variables to sets of indicators
model_data
```

This creates the `model_data` dataframe that has 67 columns.

![ec2](lab8/Screenshot%202023-10-06%20175545.png)

5. The following code removes the the economic features and duration from the data as they would need to be forecasted with high precision to use as inputs in future predictions.

```
model_data = model_data.drop(
    ["duration", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed"],
    axis=1,
)
```

### Step 5: Split Data into training, validation and test

1. The following code splits the dataset into training (70%), validation (20%), and test (10%).
	- The first block of code shuffles model_data using the sample method with frac=1 (which means to sample the whole dataset) and a random seed of 1729. The shuffled data is then split into three segments. 
	- For each of the datasets (train, validation, and test), the code reorders the columns so that the target column y_yes is the first column, and then it drops the y_no and y_yes columns from the rest. It then saves the reordered data to a CSV file. This is done for compatibility reasons as Amazon SageMaker’s XGBoost algorithm expects the first column to be the target variable and the CSV should not include headers. 

```
train_data, validation_data, test_data = np.split(
    model_data.sample(frac=1, random_state=1729),
    [int(0.7 * len(model_data)), int(0.9 * len(model_data))],
)

pd.concat([train_data["y_yes"], train_data.drop(["y_no", "y_yes"], axis=1)], axis=1).to_csv(
    "train.csv", index=False, header=False
)
pd.concat(
    [validation_data["y_yes"], validation_data.drop(["y_no", "y_yes"], axis=1)], axis=1
).to_csv("validation.csv", index=False, header=False)

pd.concat([test_data["y_yes"], test_data.drop(["y_no", "y_yes"], axis=1)], axis=1).to_csv(
    "test.csv", index=False, header=False
)
```

2. The newly created data is the uploaded to the S3 bucket we created earlier.

```
boto3.Session().resource("s3").Bucket(bucket).Object(
    os.path.join(prefix, "train/train.csv")
).upload_file("train.csv")
boto3.Session().resource("s3").Bucket(bucket).Object(
    os.path.join(prefix, "validation/validation.csv")
).upload_file("validation.csv")
```

![ec2](lab8/Screenshot%202023-10-06%20160242.png)

### Step 6: Setup Hyperparameter Optimization

1. The following code sets up a configuration for a hyperparameter tuning job:
	- ParameterRanges: This specifies the hyperparameters you want to tune and their possible values.
	- CategoricalParameterRanges: This would contain parameters that have categorical values, but it's empty as we have transformed all the categorical variables into indicator variables in the previous steps.
	- ContinuousParameterRanges: This contains hyperparameters that can take any real value within a range.
	- ResourceLimits: This part specifies how many training jobs can be created in total (MaxNumberOfTrainingJobs) and how many of them can run in parallel (MaxParallelTrainingJobs).
	- Strategy: The strategy used for the tuning job. In this case, the "Bayesian" strategy is used, which is a popular method for hyperparameter optimization. It builds a probability model of the objective function and uses it to select the most promising hyperparameters to evaluate in the true objective function.
	- HyperParameterTuningJobObjective: This is where you define the metric you want to optimize. The goal here is to maximize the "validation:auc" (Area Under the Curve for the validation set).

```
tuning_job_name = f"{student_id}-xgboost-tuningjob-01"

print(tuning_job_name)

tuning_job_config = {
    "ParameterRanges": {
        "CategoricalParameterRanges": [],
        "ContinuousParameterRanges": [
            {
                "MaxValue": "1",
                "MinValue": "0",
                "Name": "eta",
            },
            {
                "MaxValue": "10",
                "MinValue": "1",
                "Name": "min_child_weight",
            },
            {
                "MaxValue": "2",
                "MinValue": "0",
                "Name": "alpha",
            },
        ],
        "IntegerParameterRanges": [
            {
                "MaxValue": "10",
                "MinValue": "1",
                "Name": "max_depth",
            }
        ],
    },
    "ResourceLimits": {"MaxNumberOfTrainingJobs": 2, "MaxParallelTrainingJobs": 2},
    "Strategy": "Bayesian",
    "HyperParameterTuningJobObjective": {"MetricName": "validation:auc", "Type": "Maximize"},
}
```

2. The hyperparameter tuning job will launch training jobs to find an optimal configuration of hyperparameters. These training jobs should be configured using the SageMaker CreateHyperParameterTuningJob API. To configure the training jobs, we define a JSON object and pass it as the value of the TrainingJobDefinition parameter inside CreateHyperParameterTuningJob.
	- AlgorithmSpecification – The registry path of the Docker image containing the training algorithm and related metadata. We fetch the XGboost Docker image for our specic usecase. 
	- InputDataConfig: This specifies where SageMaker should fetch the training and validation data from. For both data sources, the content type is CSV and data distribution is "FullyReplicated", which means each training instance will get the full dataset.
	- OutputDataConfig: This defines where SageMaker should store the output of the training job (e.g., model artifacts). The output will be stored in the specified S3 bucket under an "output" prefix.
	- ResourceConfig: Specifies the infrastructure to be used for the training job. In this case, a single ml.m5.xlarge instance with a volume size of 10GB.
	- RoleArn: This is the Amazon Resource Name (ARN) of the IAM role that SageMaker can assume to perform tasks on your behalf. We make use of the ARN of the SageMakerRole that we retreived earlier. 
	- StaticHyperParameters: These are hyperparameters specific to the XGBoost algorithm that will remain fixed during training. For instance, objective is set to "binary:logistic", which is suitable for binary classification problems.
	- StoppingCondition: This specifies that the training job should be terminated if it runs for more than 43200 seconds (or 12 hours).


```
from sagemaker.image_uris import retrieve
# Use XGBoost algorithm for training
training_image = retrieve(framework="xgboost", region=region, version="latest")

s3_input_train = "s3://{}/{}/train".format(bucket, prefix)
s3_input_validation = "s3://{}/{}/validation/".format(bucket, prefix)

training_job_definition = {
    "AlgorithmSpecification": {"TrainingImage": training_image, "TrainingInputMode": "File"},
    "InputDataConfig": [
        {
            "ChannelName": "train",
            "CompressionType": "None",
            "ContentType": "csv",
            "DataSource": {
                "S3DataSource": {
                    "S3DataDistributionType": "FullyReplicated",
                    "S3DataType": "S3Prefix",
                    "S3Uri": s3_input_train,
                }
            },
        },
        {
            "ChannelName": "validation",
            "CompressionType": "None",
            "ContentType": "csv",
            "DataSource": {
                "S3DataSource": {
                    "S3DataDistributionType": "FullyReplicated",
                    "S3DataType": "S3Prefix",
                    "S3Uri": s3_input_validation,
                }
            },
        },
    ],
    "OutputDataConfig": {"S3OutputPath": "s3://{}/{}/output".format(bucket, prefix)},
    "ResourceConfig": {"InstanceCount": 1, "InstanceType": "ml.m5.xlarge", "VolumeSizeInGB": 10},
    "RoleArn": sagemaker_role,
    "StaticHyperParameters": {
        "eval_metric": "auc",
        "num_round": "1",
        "objective": "binary:logistic",
        "rate_drop": "0.3",
        "tweedie_variance_power": "1.4",
    },
    "StoppingCondition": {"MaxRuntimeInSeconds": 43200},
}
```

3. This code snippet is launching a hyperparameter tuning job on Amazon SageMaker.

```
#Launch Hyperparameter Tuning Job
smclient.create_hyper_parameter_tuning_job(
    HyperParameterTuningJobName=tuning_job_name,
    HyperParameterTuningJobConfig=tuning_job_config,
    TrainingJobDefinition=training_job_definition,
)
```

4. We can log into the sagemaker service on the AWS console. Here we can see all the running training instances. 

![ec2](lab8/Screenshot%202023-10-06%20160541.png)

5. The hyperparameter optimization failed and we got the message "ClientError: Non-numeric value 'F' found in the header line 'False,54,3,999,0,1,0,False,False,False,False,False...' of file 'train.csv'". In order to resolve this, we convert all the True/False in the dataset to 1/0 using the following code.

```
bool_cols = model_data.select_dtypes(include=['bool']).columns
model_data[bool_cols] = model_data[bool_cols].astype(int)
```

![ec2](lab8/Screenshot%202023-10-06%20163316.png)

6. Running the code again, we succeed this time and we can view the best training job hyperparaneters either in the AWS console or in the s3://YOUR_BUCKET/sagemaker/22489437-hpo-xgboost-dm/output.

![ec2](lab8/Screenshot%202023-10-06%20170836.png)

![ec2](lab8/Screenshot%202023-10-06%20170328.png)

## Lab 9: More AI

### Section 1: Detect Languages from text

1. We run the example code and confirm the output:

```
import boto3
client = boto3.client('comprehend')

# Detect Entities
response = client.detect_dominant_language(
    Text="The French Revolution was a period of social and political upheaval in France and its colonies beginning in 1789 and ending in 1799.",
)

print(response['Languages'])
```
![ec2](lab9/Screenshot%202023-10-12%20171650.png)

2. We modify the code to use a script to recognize different languages and returnt he message in the format: "<predicted_language> detected with xx% confidence"
	- `client = boto3.client('comprehend')`: This initializes a client to interact witht he Amazon Comprehend service 
	- `argParser = argparse.ArgumentParser()`: The first section of the main function uses the argparse library to allow the program to accept command line input of plaintext which we will process.
	- `response = client.detect_dominant_language`: We call the detect_dominant_language method of the Comprehend client to determine the dominant language of the plaintext. The result is stored in the response variable.
	- `lang_code = response['Languages'][0]['LanguageCode']`: This line extracts the language code (e.g., "en" for English) of the detected dominant language from the response.
	- `lang = pycountry.languages.get(alpha_2=lang_code).name`: Since we want the language name in full we use the pycountry library to convert the language into a full language name.
	- `conf = str(int(response['Languages'][0]['Score'] * 100))`: Here, the confidence score (a float between 0 and 1) returned by the Comprehend service is multiplied by 100 to convert it into a percentage and then rounded to an integer.

```
client = boto3.client('comprehend')

def main(argv):
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-t", "--plaintext", type=str)

    args = argParser.parse_args()
    plaintext = args.plaintext

    response = client.detect_dominant_language(
        Text=plaintext
    )

    lang_code = response['Languages'][0]['LanguageCode']
    lang = pycountry.languages.get(alpha_2=lang_code).name

    conf = str(int(response['Languages'][0]['Score'] * 100))

    print(lang +" detected with "+ conf + "%"+ " confidence")

    return 0
```

3. The results of the script on the 4 provided samples can be seen below:

![ec2](lab9/Screenshot%202023-10-12%20181238.png)

![ec2](lab9/Screenshot%202023-10-12%20181259.png)

![ec2](lab9/Screenshot%202023-10-12%20181318.png)

![ec2](lab9/Screenshot%202023-10-12%20181340.png)

### Section 2: Sentiment Analysis 

1. The code below performs sentiment analysis on the supplied plaintext:
	- `client = boto3.client('comprehend')`: This initializes a client to interact witht he Amazon Comprehend service 
	- `argParser = argparse.ArgumentParser()`: The first section of the main function uses the argparse library to allow the program to accept command line input of plaintext which we will process.
	- `response = client.detect_dominant_language`: We call the detect_dominant_language method of the Comprehend client to determine the dominant language of the plaintext. The result is stored in the response variable.
	- `lang_code = response['Languages'][0]['LanguageCode']`: This line extracts the language code (e.g., "en" for English) of the detected dominant language from the response. This is required for the following function call. 
	- `sen = client.detect_sentiment(Text=plaintext, LanguageCode=lang_code)`: We use the detect_sentiment method of the Comprehend client to determine the sentiment of the text. This method requires both the text and its language code. The sentiment result (e.g., "POSITIVE", "NEGATIVE", "NEUTRAL", "MIXED") is then extracted from the response.

```
client = boto3.client('comprehend')

def main(argv):
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-t", "--plaintext", type=str)

    args = argParser.parse_args()
    plaintext = args.plaintext

    response = client.detect_dominant_language(
        Text=plaintext
    )

    lang_code = response['Languages'][0]['LanguageCode']
    lang = pycountry.languages.get(alpha_2=lang_code).name

    sen = client.detect_sentiment(Text=plaintext, LanguageCode = lang_code)
    sentiment = sen['Sentiment']
    print("The sentiment of the %s text is %s" % (lang, sentiment))

    return 0
```

2. The results of the script on the 4 provided samples can be seen below:

![ec2](lab9/Screenshot%202023-10-12%20181411.png)

![ec2](lab9/Screenshot%202023-10-12%20181432.png)

![ec2](lab9/Screenshot%202023-10-12%20181450.png)

![ec2](lab9/Screenshot%202023-10-12%20181506.png)

### Section 3: Entity detection

1. The code below performs key phrase on the supplied plaintext:
	- `client = boto3.client('comprehend')`: This initializes a client to interact witht he Amazon Comprehend service 
	- `argParser = argparse.ArgumentParser()`: The first section of the main function uses the argparse library to allow the program to accept command line input of plaintext which we will process.
	- `response = client.detect_dominant_language`: We call the detect_dominant_language method of the Comprehend client to determine the dominant language of the plaintext. The result is stored in the response variable.
	- `lang_code = response['Languages'][0]['LanguageCode']`: This line extracts the language code (e.g., "en" for English) of the detected dominant language from the response. This is required for the following function call. 
	- `ent = client.detect_entities(Text=plaintext, LanguageCode = lang_code)`: With the detect_entities method of the Comprehend client, the script determines the entities present in the text.

```
client = boto3.client('comprehend')

def main(argv):
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-t", "--plaintext", type=str)

    args = argParser.parse_args()
    plaintext = args.plaintext

    response = client.detect_dominant_language(
        Text=plaintext
    )

    lang_code = response['Languages'][0]['LanguageCode']

    ent = client.detect_entities(Text=plaintext, LanguageCode = lang_code)
    entities = ent['Entities']
    
    if not entities:
        print('There are no entities in the text')
        return
    print("The entities in the text are:")
    for ent in entities:
       print(ent['Text'] + ' is ' + ent['Type'])

    return 0
```

2. The results of the script on the 4 provided samples can be seen below:

![ec2](lab9/Screenshot%202023-10-12%20182446.png)

![ec2](lab9/Screenshot%202023-10-12%20182513.png)

![ec2](lab9/Screenshot%202023-10-12%20182533.png)

![ec2](lab9/Screenshot%202023-10-12%20182552.png)

3. Entities refer to chunks of text that can be classified into groups based on their semantic meaning. These often represent real-world objects such as people, places, organizations, dates, and other named objects or concepts. In the context of Natural Language Processing (NLP), identifying entities can help in understanding the content and context of a document.

### Section 4: Keyphrase detection

1. The code below performs key phrase on the supplied plaintext:
	- `client = boto3.client('comprehend')`: This initializes a client to interact witht he Amazon Comprehend service 
	- `argParser = argparse.ArgumentParser()`: The first section of the main function uses the argparse library to allow the program to accept command line input of plaintext which we will process.
	- `response = client.detect_dominant_language`: We call the detect_dominant_language method of the Comprehend client to determine the dominant language of the plaintext. The result is stored in the response variable.
	- `lang_code = response['Languages'][0]['LanguageCode']`: This line extracts the language code (e.g., "en" for English) of the detected dominant language from the response. This is required for the following function call. 
	- `key_phrases = client.detect_key_phrases(Text=plaintext, LanguageCode = lang_code)`: The detect_key_phrases method of the Comprehend client is called to find the key phrases in the text. These key phrases are then stored in the key_phrase variable.

```
client = boto3.client('comprehend')

def main(argv):
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-t", "--plaintext", type=str)

    args = argParser.parse_args()
    plaintext = args.plaintext

    response = client.detect_dominant_language(
        Text=plaintext
    )

    lang_code = response['Languages'][0]['LanguageCode']

    key_phrases = client.detect_key_phrases(Text=plaintext, LanguageCode = lang_code)
    key_phrase = key_phrases['KeyPhrases']
    if not key_phrase:
        print('There are no key phrases in the text')
        return
    print("The key phrases in the text are:")
    for kp in key_phrase:
       print(kp['Text'])

    return 0
```

2. The results of the script on the 4 provided samples can be seen below:

![ec2](lab9/Screenshot%202023-10-12%20183616.png)

![ec2](lab9/Screenshot%202023-10-12%20183639.png)

![ec2](lab9/Screenshot%202023-10-12%20183659.png)

![ec2](lab9/Screenshot%202023-10-12%20183718.png)

3. Keyphrases, often termed as "keywords" or "key terms", refer to words or groups of words within a text that are deemed significant or representative of the primary content and topics of that text. The identification of keyphrases is crucial in various fields of text analysis, as these phrases help in summarizing, categorizing, and understanding the primary themes or topics of the text.

### Section 5: Detecting Syntax

1. The code below performs key phrase on the supplied plaintext:
	- `client = boto3.client('comprehend')`: This initializes a client to interact witht he Amazon Comprehend service 
	- `argParser = argparse.ArgumentParser()`: The first section of the main function uses the argparse library to allow the program to accept command line input of plaintext which we will process.
	- `response = client.detect_dominant_language`: We call the detect_dominant_language method of the Comprehend client to determine the dominant language of the plaintext. The result is stored in the response variable.
	- `lang_code = response['Languages'][0]['LanguageCode']`: This line extracts the language code (e.g., "en" for English) of the detected dominant language from the response. This is required for the following function call. 
	- `syn = client.detect_syntax(Text=plaintext, LanguageCode = lang_code)`: The detect_syntax method of the Comprehend client identifies the syntactic elements (parts of speech) in the text, and the results are stored in the syntax variable.

```
client = boto3.client('comprehend')

def main(argv):
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-t", "--plaintext", type=str)

    args = argParser.parse_args()
    plaintext = args.plaintext

    response = client.detect_dominant_language(
        Text=plaintext
    )

    lang_code = response['Languages'][0]['LanguageCode']

    syn = client.detect_syntax(Text=plaintext, LanguageCode = lang_code)
    syntax = syn['SyntaxTokens']
    print("The syntax in the text are:")
    for s in syntax:
       print(s['Text'] +' is '+ s['PartOfSpeech']['Tag'] + '\t')

    return 0
```

2. The results of the script on the 4 provided samples can be seen below:

![ec2](lab9/Screenshot%202023-10-12%20184028.png)

![ec2](lab9/Screenshot%202023-10-12%20184052.png)

![ec2](lab9/Screenshot%202023-10-12%20184112.png)

![ec2](lab9/Screenshot%202023-10-12%20184134.png)

3. Syntax refers to the arrangement of words and phrases in sentences to create well-formed and meaningful statements in a particular language. In the context of linguistics and natural language processing (NLP), syntax is the study of the rules, principles, and processes that govern the structure of sentences. Essentially, it's about how words come together to form coherent and grammatically correct sentences.

### Section 6: Creating S3 bucket

1. We create an S3 bucket named `22489437-cloudstorage` using the aws console

![ec2](lab9/Screenshot%202023-10-12%20204442.png)

2. We upload 4 files to the newly created S3 bucket.

![ec2](lab9/Screenshot%202023-10-12%20204442.png)

`urban_times_square.jpg`: This simulates an image of an urban setting

![ec2](lab9/urban_times_square.jpg)

`person_on_beach.jpeg`: This is a group of people on the beach wearing swimsuits. This will be used to test detection of explicit and suggestive adult content. 

![ec2](lab9/person_on_beach.jpeg)

`people_with_faces.png`: This is 5 people with different races to test the analysis of facial attributes. 

![ec2](lab9/people_with_faces.png)

`image_with_text.jpg`: This is an image with text to test extraction of text from images.

![ec2](lab9/image_with_text.jpg)

3. The paths to these images will be stored in a dictionary:

```
bucket_images=[
    {
        'S3Object': {
            'Bucket': '22489437-cloudstorage',
            'Name': 'urban_times_square.jpg'
        },
    },
    {
        'S3Object': {
            'Bucket': '22489437-cloudstorage',
            'Name': 'person_on_beach.jpeg'
        },
    },    
    {
        'S3Object': {
            'Bucket': '22489437-cloudstorage',
            'Name': 'people_with_faces.png'
        },
    },
    {
        'S3Object': {
            'Bucket': '22489437-cloudstorage',
            'Name': 'image_with_text.jpg'
        },
    },        
]
```

### Section 7: Label Recognition

1. The code below performs label recognition on the images listed in section 6:
	- A rekognition client object is created using the boto3 library. This object is used to make calls to the Amazon Rekognition service.
	- For each image, the code tries to detect labels using the Amazon Rekognition's detect_labels method. This method identifies objects, people, text, scenes, etc., in the image.
	- The detected labels are printed

```
client = boto3.client('rekognition')

def main(argv):
    for image in bucket_images:
        image_name = image['S3Object']['Name']        
        try:
            response = client.detect_labels(Image=image, MaxLabels=3, MinConfidence=0.95)
            print('Labels detected in %s:' % image_name)
            print(response['Labels'])
        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    return 0
```

2. The output of the code is shown below, each of the detected labels as well as their confidence score is shown below:

![ec2](lab9/Screenshot%202023-10-12%20210734.png)

### Section 8: Image Moderation

1. The code below performs image moderation on the images listed in section 6:
	- A rekognition client object is created using the boto3 library. This object is used to make calls to the Amazon Rekognition service.
	- For each image, this code detects moderation labels for the image. Amazon Rekognition's detect_moderation_labels method is used to identify potentially unsafe content in the image. This can help filter out content that violates certain guidelines or standards.

```
client = boto3.client('rekognition')

def main(argv):
    for image in bucket_images:
        image_name = image['S3Object']['Name']        
        try:
            response = client.detect_moderation_labels(Image=image)
            print('Moderated for %s:' % image_name)
            print(response['ModerationLabels'])
            print('\n')
        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    return 0
```

2. The output of the code is shown below. As seen, only `person_on_beach.jpeg` was flagged due to suggestive female swimwear. The other images did not trigger the moderation tool.

![ec2](lab9/Screenshot%202023-10-12%20211107.png)

### Section 9: Facial Analysis 

1. The code below performs facial analysis on the images listed in section 6:
	- A rekognition client object is created using the boto3 library. This object is used to make calls to the Amazon Rekognition service.
	- For each image, this code uses the detect_faces method of the Rekognition client to identify faces in the image.
	- Using the detect_faces method, we can detect faces in an image, and for each detected face, we can get details about facial attributes, landmarks, pose, quality, and more. This can be useful for a range of applications, from simple face detection to more advanced use cases like emotion analysis.

```
client = boto3.client('rekognition')

def main(argv):
    for image in bucket_images:
        image_name = image['S3Object']['Name']        
        try:
            response = client.detect_faces(Image=image)
            print('facial analysis for %s:' % image_name)
            print(response['FaceDetails'])
            print('\n')
        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    return 0
```

2. The output of the code is shown below. For each detected feature, the coordinates of the bounding box is provided.

![ec2](lab9/Screenshot%202023-10-12%20211247.png)

### Section 10: Text Extraction

1. The code below performs text extraction on the images listed in section 6:
	- A rekognition client object is created using the boto3 library. This object is used to make calls to the Amazon Rekognition service.
	- For each image, the code uses the detect_text method of the Rekognition client to identify and extract text present in the image. This function can recognize and extract both printed and handwritten text.

```
client = boto3.client('rekognition')
def main(argv):
    for image in bucket_images:
        image_name = image['S3Object']['Name']        
        try:
            response = client.detect_text(Image=image)
            print('text extraction for %s:' % image_name)
            print(response['TextDetections'])
            print('\n')
        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    return 0
```

2. The output of the code is shown below. As seen in the screenshot, the first line of the text `How To` was detected.

![ec2](lab9/Screenshot%202023-10-12%20211458.png)
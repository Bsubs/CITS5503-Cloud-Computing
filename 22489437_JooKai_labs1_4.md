# Lab Report 1-4

*Author*: Joo Kai Tay (22489437)

## Lab 1: Introduction & Setup

### Section 1: Creation of AWS Account

1. Navigate to https://489389878001.signin.aws.amazon.com/console and login using the provided username and password, changing the password to a secure one on login.
<br/><br/>
![AWS login screen](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20210712.jpg)

2. Navigate to the security credentials tab within IAM. Here, the user can view their details such as their ARN and canonical ID.
<br/><br/>
![Security credentials screen](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211014.jpg)

3. Select the option to create an access key for access to programmatic calls to AWS from the AWS CLI.
<br/><br/>
![Create Access key](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211030.jpg)

4. Select the option for use with the command line interface (CLI)
<br/><br/>
![Access key best practices](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211049.jpg)

5. Give the access key a meaningful description tag. This will be useful in the event that you create multiple access keys. The tag should describe the purpose of the key and where it will be used.
<br/><br/>
![Access key tag](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211109.jpg)

6. Once the key has been successfully created, download the access key and secret access key in the .csv file and store it securely. If the key is not stored securely, any user with access to the key can use it to create and use resources associated with your account. For privacy purposes, the key is censored in this image.
<br/><br/>
![Successful creation of access key](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211138.jpg)

### Section 2: VM Setup 

The Ubuntu virtual machine was already setup prior to the commencement of this unit. The specifications can be found in the attached image.
<br/><br/>
![Virtual machine specifications](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211237.jpg)

### Section 3: Software Setup

1. Python 3.10.6 and pip3 were already installed on the machine prior to the commencement of the unit.
<br/><br/>
![Python installation](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211353.jpg)

2. The AWS CLI was installed and configured using the access key generated as part of step 6 in section 1
<br/><br/>
![Configuring AWS](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211531.jpg)

3. Boto3 was installed as per the instructions. Boto3 is the AWS SDK for python and it provides a python API for AWS infrastructure services.
<br/><br/>
![Installing boto3](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211547.jpg)

### Section 4: Exploring and testing the environment 

1. Running the command `aws ec2 describe-regions --output table` is used to retrieve information about the available AWS regions for the EC2 service and display the output in a tabular format. The output of the command is shown below:
<br/><br/>
![Command output](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211610.jpg)


2. We can retrieve similar information using Python3 by running the following commands. This returns and prints the same data as before. However, this data is not tabulated.
```
    python3
    >>> import boto3
    >>> ec2 = boto3.client('ec2')
    >>> response = ec2.describe_regions()
    >>> print(response)
```
<br/><br/>
![python method](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211626.jpg)  

3. The following code allows us to tabulate the data and print out only the columns containing the endpoint and RegionName:
```
    import boto3
    
    # Get the region data from boto3
    ec2 = boto3.client('ec2')
    response = ec2.describe_regions()
    
    # Print the header for the columns
    print(f"{'Endpoint':<40} {'RegionName'}")
    
    # Print each region's endpoint and region name
    for region in response['Regions']:
        print(f"{region['Endpoint']:<40} {region['RegionName']}")
```
The execution of this code (stored in a filed named cits5503_lab1.py) is shown below:
<br/><br/>
![python file](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211644.jpg)  


## Lab 2: EC2 and Docker

### Section 1: Creating an EC2 instance using awscli

1. Using the command `aws ec2 create-security-group --group-name <student number>-sg --description "security group for development environment"`, a security group is created. The security group ID is noted for use in future steps.
<br/><br/>
![Creating security group](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20164650.jpg)

2. The following command is used to authorise inbound traffic for ssh: `aws ec2 authorize-security-group-ingress --group-name <student number>-sg --protocol tcp --port 22 --cidr 0.0.0.0/0`. Some things to note about this command:
    a. The TCP protocol is specified using `--protocol tcp --port 22` and the default port 22 is selected.
    b. The command `--cidr 0.0.0.0/0` is used as a wildcard. This allows all IP addresses to connect to SSH port that was opened. This is considered a major security risk as it allows anyone on the internet to attempt SSH connections to instances associated with this security group.
<br/><br/>
![Authorise security group](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20164856.jpg)

3. The command `aws ec2 create-key-pair --key-name <student number>-key --query 'KeyMaterial' --output text > <student number>-key.pem` creates a key pair that will allow a user to ssh to the EC2 instance. The private key is stored in the output file `22489437-key.pem`
<br/><br/>
![Creating private key](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165022.jpg)

4. The private key file is moved into the SSH directory and its permissions are changed to be read only for the owner of the file, and no permissions for others.
<br/><br/>
![changing permissions](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165358.jpg)

5. The command ` aws ec2 run-instances --image-id ami-d38a4ab1 --security-group-ids <student number>-sg --count 1 --instance-type t2.micro --key-name <student number>-key --query 'Instances[0].InstanceId' is used to create the EC2 instance. The command returns the instance ID of the created instance.
<br/><br/>
![creating EC2 instance](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165508.jpg)

6. Using the instance ID from step 5, a tag is added to the instance.
<br/><br/>
![create tag](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165629.jpg)

7. The public IP address of this instance is determined using the command `aws ec2 describe-instances --instance-ids i-<instance id from above> --query 'Reservations[0].Instances[0].PublicIpAddress'`
<br/><br/>
![public IP](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165711.jpg)

8. Using the private key file from step 3 and the IP address from step 7, the user is able to SSH into the EC2 instance.
<br/><br/>
![ssh](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165859.jpg)

9. The created instance can be viewed from the AWS console
<br/><br/>
![AWS console](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20170050.jpg)

10. The instance can be terminated from the awscli using the following command: `aws ec2 terminate-instances --instance-ids i-<your instance id>`
<br/><br/>
![terminating the instance](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20170214.jpg)


## Section 2: Creating an AWS EC2 Instance with Python Boto Script

1. The following code is used to replicate steps 1-7 from section 1:
```
    import boto3
    import os
    import shutil
    
    student_number = "22489437"
    region = "ap-southeast-2"
    
    # Initialize the EC2 client
    ec2 = boto3.client('ec2', region)
    
    # Create a security group
    response = ec2.create_security_group(
        GroupName=f"{student_number}-sg",
        Description="security group for development environment"
    )
    security_group_id = response['GroupId']
    
    # Authorize inbound SSH traffic for the security group
    ec2.authorize_security_group_ingress(
        GroupId=security_group_id,
        IpProtocol="tcp",
        FromPort=22,
        ToPort=22,
        CidrIp="0.0.0.0/0"
    )
    
    # Create a key pair and save the private key to a file
    response = ec2.create_key_pair(KeyName=f"{student_number}-key")
    private_key = response['KeyMaterial']
    
    private_key_file = f"{student_number}-key.pem"
    # Allow writing to the private key file
    os.chmod(private_key_file, 0o666)
    with open(private_key_file, 'w') as key_file:
        key_file.write(private_key)
    
    # Set the correct permissions for the private key file
    os.chmod(private_key_file, 0o400)
    
    # Copy the private key file to ~/.ssh directory
    ssh_directory = os.path.expanduser("~/.ssh")
    if not os.path.exists(ssh_directory):
        os.makedirs(ssh_directory)
    shutil.copy(private_key_file, ssh_directory)
    
    # Launch an EC2 instance
    response = ec2.run_instances(
        ImageId="ami-d38a4ab1",
        SecurityGroupIds=[security_group_id],
        MinCount=1,
        MaxCount=1,
        InstanceType="t2.micro",
        KeyName=f"{student_number}-key"
    )
    instance_id = response['Instances'][0]['InstanceId']
    
    # Wait for the instance to be up and running
    ec2.get_waiter('instance_running').wait(InstanceIds=[instance_id])
    
    # Describe the instance to get its public IP address
    response = ec2.describe_instances(InstanceIds=[instance_id])
    public_ip_address = response['Reservations'][0]['Instances'][0]['PublicIpAddress']
    print(f"Instance created successfully with Public IP: {public_ip_address}")
```
2. An error was shown when running the script. This was due to the existing key pair that was created in step 1. In order to resolve this error, we head into the AWS console and delete the previously created key pair.
<br/><br/>
![key pair error](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123521.jpg)
<br/><br/>
![deleting key pair](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123557.jpg)

4. A second error was shown when running the script. This was due to the existing security group that was created in step 1. In order to resolve this error, we head into the AWS console and delete the previously created security group.
<br/><br/>
![security group error](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123540.jpg)
<br/><br/>
![deleting security group](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123647.jpg)

5. The script ran successfully and returned he public IP of the created EC2 instance:
<br/><br/>
![public IP](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20125808.jpg)

6. The details of the EC2 instance on the AWS console:
<br/><br/>
![details](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20125823.jpg)

7. This instance was terminated using the AWS console instead of the AWSCLI, demonstrating the second way to terminate an EC2 instance:
<br/><br/>
![terminate](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131032.jpg)


### Section 3: Using Docker

1. Docker was installed using the command `sudo apt install docker.io -y`
<br/><br/>
![installing docker](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131356.jpg)

2. Enabling docker and checking the version installed:
<br/><br/>
![check docker](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131558.jpg)

3. Creating index.html:
<br/><br/>
![index](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131742.jpg)

4. Creating the Dockerfile
<br/><br/>
![dockerfile](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133452.jpg)

5. Building the docker image from the dockerfile and index.html
<br/><br/>
![build](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133827.jpg)

6. Running the docker container:
<br/><br/>
![run](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133907.jpg)

7. Confirming the "Hello World!" output:
<br/><br/>
![confirm](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133934.jpg)

8. Viewing what is running:
<br/><br/>
![view](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20134001.jpg)

9. Terminating the container:
<br/><br/>
![terminate](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20134036.jpg)
<br/><br/>
![terminate](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20134052.jpg)


## Lab 3: Configure S3 Buckets

1. Preparation:
    a. Downloading cloudstorage.py
        <br/><br/>
       ![download](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-13%20152943.png)
   
   b. Creating rootdir
        <br/><br/>
       ![rootdir](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-13%20153423.png)
   
   c. Creating subdir
       <br/><br/>
       ![subdir](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-13%20153612.png)
   
3. Editing cloudstorage.py to take the initialise argument. The following code segment uses the argparse library to allow the argparse library to take 2 arguments `-i` or `-initialise`. Using either of these 2 arguments will set the `bucketFlag` variable to True. If `bucketFlag` is True, we will attempt to create a bucket in S3. We also catch any exceptions such as `BucketAlreadyExists` and `BucketAlreadyOwnedByYou`. 
   ```
       	argParser = argparse.ArgumentParser()
    	argParser.add_argument("-i", "--initialise", action='store_true', help="create bucket on s3")
    	args = argParser.parse_args()
    	bucketFlag = args.initialise
    	# Insert code to create bucket if not there
    	if(bucketFlag):
    		try:
    			response = s3.create_bucket(Bucket=ROOT_S3_DIR, CreateBucketConfiguration=bucket_config)
    			print("Bucket created successfully:", ROOT_S3_DIR)
    			print(response)
    		except s3.exceptions.BucketAlreadyExists:
    			print("Bucket already exists:", ROOT_S3_DIR)
    		except s3.exceptions.BucketAlreadyOwnedByYou:
    			print("Bucket already owned by you:", ROOT_S3_DIR)
    		except Exception as error:
    			print("An error occurred:", error)
    ```
4. The `upload_file()` method found in `cloudstoraage.py` was modified to add the code required to upload files to S3. The path of the file in S3 is constructed by adding the `ROOT_S3_DIR` which was created in step 2 to the folder path and file name. The file is then uploaded to S3 using the `s3.upload_file()` command.
      ```
          def upload_file(folder_name, file, file_name):
        	# Construct the path for files to be uploaded
        	s3Path = f'{ROOT_S3_DIR}/{folder_name}/{file_name}'
        
        	print("Uploading %s" % file)
        	
        	# Upload the file to S3
        	try:
        		s3.upload_file(file, ROOT_S3_DIR, s3Path)
        		print(f"Uploaded {file_name} to S3")
        	except Exception as e:
        		print(f"Error uploading {file_name}: {e}")
      ```
   <br/><br/>
        ![uploads](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20115517.png)
       ![uploads2](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-17%20175214.png)
       ![uploads3](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-17%20175233.png)
       ![uploads4](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-17%20175243.png)
   
6. The following code which is saved in restorefromcloud.py reads the content of the s3 bucket using the command `s3.list_objects` and writes them to the local directory. If the folder structure does not exist in the local directory, the code uses `os.makedirs` to recreate the folder structure from s3 in the local directory.
    ```
        import os
        import boto3
        ROOT_DIR = '.'
        ROOT_S3_DIR = '22489437-cloudstorage'
        REGION = 'ap-southeast-2'
        
        s3 = boto3.client("s3", region_name=REGION)
        bucket_config = {'LocationConstraint': 'ap-southeast-2'}
        
        for key in s3.list_objects(Bucket = ROOT_S3_DIR)['Contents']:
        	if not os.path.exists(os.path.dirname(key['Key'])):
        		os.makedirs(os.path.dirname(key['Key']))
        		
        	print("Downloading %s" % key['Key'])
        	s3.download_file(ROOT_S3_DIR, key['Key'],  key['Key'])
    ```
    <br/><br/>
    ![downloads](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20131746.png)
   
7. Setting up DynamoDB locally
        <br/><br/>
       ![createdir](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132006.png)
       <br/><br/>
        ![installjre](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132014.png)
       <br/><br/>
        ![downloaddb](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132048.png)
       <br/><br/>
        ![start](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132216.png)
       
8. Creating a table with the following attributes:
        ```
            CloudFiles = {
                    'userId',
                    'fileName',
                    'path',
                    'lastUpdated',
        	    'owner',
                    'permissions'
                    }
                )
        ```
   <br/><br/>
    ![createtable](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20143839.png)
   
7. The following code which is saved in storeinfo.py is used to get the file information from s3 and put it into the DynamoDB table created in step 6.
       
```
            import boto3

            ROOT_S3_DIR = '22489437-cloudstorage'  
            REGION = 'ap-southeast-2'  
            TABLE = 'CloudFiles'  
            
            # Initialize AWS resources
            dynamodb = boto3.resource('dynamodb', region_name=REGION, endpoint_url='http://localhost:8000')  
            table = dynamodb.Table(TABLE)  
            s3 = boto3.client('s3', region_name=REGION)  
            
            # List objects in the specified S3 bucket
            response = s3.list_objects(Bucket=ROOT_S3_DIR)
            
            # Extract relevant information from the S3 response
            userId = str(response['Contents'][0]['Owner']['ID'])
            owner = response['Contents'][0]['Owner']['DisplayName']
            permission = s3.get_bucket_acl(Bucket=ROOT_S3_DIR)['Grants'][0]['Permission']
            
            
            # Iterate through each object in the S3 bucket
            for content in response['Contents']:
                # Create an item for DynamoDB with relevant attributes
                item = {
                    'userId': userId,
                    'fileName': content['Key'].split('/')[-1],  # Extract the file name from the full S3 object key
                    'path': content['Key'],
                    'lastUpdated': str(content['LastModified']),
                    'owner': owner,
                    'permissions': permission
                }
                
                print('Putting the following item into DynamoDB table:\n', item, '\n')
                
                # Add the item to the DynamoDB table
                table.put_item(Item=item)
```
<br/><br/>
8. The code is run which stores the details of the two files into the `CloudFiles` table:
<br></br>
![](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20145833.png)
<br></br>

9. The `CloudFiles` table is scanned using the command `aws dynamodb scan --table-name CloudFiles --endpoint-url http://localhost:8000`. This reveals the details of the two files which were added in step 8.
<br></br>
![](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20145934.png)
<br></br>
## Lab 4: Encryption

### Section 1: Applying Policy to Restrict Permissions

1. The following code was used to apply a policy to allow only my username (22489437@student.uwa.edu.au) to access to the S3 bucket identified by `arn:aws:s3:::22489437-cloudstorage` as well as the objects inside the bucket.
```
ROOT_DIR = '.'
ROOT_S3_DIR = '22489437-cloudstorage'
REGION = 'ap-southeast-2'
student_number = '22489437'

s3 = boto3.client("s3", region_name=REGION)
bucket_config = {'LocationConstraint': 'ap-southeast-2'}

policy = {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowAllS3ActionsInUserFolderForUserOnly",
            "Effect": "DENY",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": [
		"arn:aws:s3:::22489437-cloudstorage/*",
		"arn:aws:s3:::22489437-cloudstorage"
		],
            "Condition": {
                "StringNotLike": {
                    "aws:username": f"{student_number}@student.uwa.edu.au"
                }
            }
        }
    ]
}

def main(argv):
	policyJson = json.dumps(policy)
	s3.put_bucket_policy(Bucket=ROOT_S3_DIR, Policy=policyJson)
	print("Updated bucket policy")
	return 0

if __name__ == "__main__":
	main(sys.argv[1:])
```
<br/><br/>
![applypolicy](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20151303.png)
<br/><br/>
2. Inspecting the bucket from the AWS console reveals the updated policy that was added to the bucket.
<br/><br/>
![AWS Policy](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-30%20110059.png)
<br/><br/>
Attempting to view this bucket from another user with username `22687382@student.uwa.edu.au` shows that they have insufficient permission to list objects in this bucket. This shows that the policy applied has it's intended effect.
<br/><br/>
![](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Image%2030-8-2023%20at%2010.40%20am.jpg)
<br/><br/>

## Section 2: AES Encryption using KMS

1. The following code creates a KMS key and attaches an alias (22489437_2) to it. The code also atatches a policy to the key which makes 22489437@student.uwa.edu.au the user and administrator. The policy, represented by the `key_policy` variable which can be found in Lab Sheet 4 is not added to this report.
```
def main(argv):
	kms_client = boto3.client('kms', region_name=REGION)
	# Create Key
	response = kms_client.create_key(
		Description='22489437_key_2',
		KeyUsage='ENCRYPT_DECRYPT',
		Origin='AWS_KMS'
	)
	key_id = response['KeyMetadata']['KeyId']
	print("KMS key id:", key_id)

	# Attack key policy from labsheet 
	kms_client.put_key_policy(
		KeyId=key_id,
		PolicyName='default',
		Policy=json.dumps(key_policy)
	)

	# Create alias for key
	kms_client.create_alias(
		AliasName='alias/22489437_2',
		TargetKeyId=key_id
	)
	print(f"Created alias '22489437' for KMS key with ID: {key_id}")

	return 0

if __name__ == "__main__":
	main(sys.argv[1:])
```
<br/><br/>
![run code](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20161347.png)

2. Inspecting the created key in the KMS console reveals the same key ID as step 1 as well as the attached key policy.
<br/><br/>
![key policy](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20161440.png)

3. The following code will encrypt the file and upload it to S3. The code uses the `kms.generate_data_key()` function in combination with the keyID from step 1 to generate a data key that will be used to encrypt the file. This will return a data key in plaintext and ciphertext. The plaintext data key is used to encrypt the file and the encrypted data key is written into the file where it can be retreived for decryption later. The encrypted file is then uploaded to S3. 
```
ROOT_DIR = '.'
ROOT_S3_DIR = '22489437-cloudstorage'
REGION = 'ap-southeast-2'
KEY_ID = '34005dc0-f101-4523-947c-ec969b05484f'
FILE_NAME = 'enc_test.txt'
NUM_BYTES_FOR_LEN = 4
s3 = boto3.client("s3", region_name=REGION)
kms = boto3.client("kms", region_name=REGION)

def encrypt_file():
	# Create Data Key
	try:
		response = kms.generate_data_key(KeyId=KEY_ID, KeySpec='AES_256')
	except ClientError as e:
		logging.error(e)

	data_key_encrypted, data_key_plaintext = response['CiphertextBlob'], base64.b64encode(response['Plaintext'])
	print("Data key encrypted:", data_key_encrypted)
	print("Data key plaintext:", data_key_plaintext)

	# Read File 
	try:
		with open(FILE_NAME, 'rb') as file:
			file_contents = file.read()
	except IOError as e:
		logging.error(e)
		return False

	# Encrypt file
	f = Fernet(data_key_plaintext)
	file_contents_encrypted = f.encrypt(file_contents)

	# Write the encrypted data key and encrypted file contents together
	try:
		with open(FILE_NAME + '.encrypted', 'wb') as file_encrypted:
			file_encrypted.write(len(data_key_encrypted).to_bytes(NUM_BYTES_FOR_LEN, byteorder='big'))
			file_encrypted.write(data_key_encrypted)
			file_encrypted.write(file_contents_encrypted)
	except IOError as e:
		logging.error(e)
		return False

	# Upload the file to S3
	file_name = FILE_NAME + '.encrypted'
	try:
		s3.upload_file(file_name, ROOT_S3_DIR, file_name, ExtraArgs={'ServerSideEncryption': "aws:kms", "SSEKMSKeyId": KEY_ID})
		print(f"Uploaded {file_name} to S3")
	except Exception as e:
		print(f"Error uploading {file_name}: {e}")
```
4. The following file named `enc_test.txt` will be the subject of the encryption and decryption for the next step
<br/><br/>
![enc](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20165638.png)
<br></br>
The output of the encryption:
![enc](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20174721.png)
<br></br>
The file can be viewed in the AWS console. Note the server-side encryption setting matching the key generated in step 1.
![file](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20172343.png)
<br></br>
5. The following code will download the file from S3 and decrypt it. The file is downloaded from S3. The encrypted data key which was placed in the file in step 4 will be read from the file and decrypted. The decrypted data key will be used to decrypt the rest of the file.
```
def decrypt_file():
	# Download file from S3
	file_name = FILE_NAME + '.encrypted'
	s3.download_file(ROOT_S3_DIR, file_name, file_name)
	print(f"Downloaded {file_name} from S3")

	# Read the encrypted file into memory
	try:
		with open(file_name, 'rb') as file:
			file_contents = file.read()
	except IOError as e:
		logging.error(e)
		return False

   	# Get encrypted data key from file
	data_key_encrypted_len = int.from_bytes(file_contents[:NUM_BYTES_FOR_LEN], byteorder='big') + NUM_BYTES_FOR_LEN
	data_key_encrypted = file_contents[NUM_BYTES_FOR_LEN:data_key_encrypted_len]

	# Decrypt data key
	response = kms.decrypt(CiphertextBlob=data_key_encrypted)
	data_key_plaintext = base64.b64encode((response['Plaintext']))

	# Decrypt file
	f = Fernet(data_key_plaintext)
	file_contents_decrypted = f.decrypt(file_contents[data_key_encrypted_len:])

	# Write the decrypted file contents
	try:
		with open(FILE_NAME + '.decrypted', 'wb') as file_decrypted:
			file_decrypted.write(file_contents_decrypted)
	except IOError as e:
		logging.error(e)
		return False

	print(f"Decrypted {FILE_NAME}")
```
The output of the decryption:
![dec](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20174738.png)
<br></br>

### Section 3: AES Encryption using local python library pycryptodome

1. The example code in `fileencrypt.py` was used for the local encryption and decryption process following the same steps as above. The file was encrypted, uploaded to S3 then downloaded and decrypted. The code below shows the main program of the program and does not include the `encrypt_file` and `decrypt-file` functions which have not been modified from the example code:
```
s3 = boto3.client("s3", region_name=REGION)
password = 'kitty and the kat'
encrypt_file(password,"enc_test.txt", out_filename="enc_test.txt.enc")
try:
	s3.upload_file("enc_test.txt.enc", ROOT_S3_DIR, "enc_test.txt.enc")
	print(f"Uploaded enc_test.txt.enc to S3")
except Exception as e:
	print(f"Error uploading enc_test.txt.enc: {e}")

s3.download_file(ROOT_S3_DIR, "enc_test.txt.enc", "enc_test.txt.enc")
print(f"Downloaded enc_test.txt.enc from S3")

decrypt_file(password, "enc_test.txt.enc", out_filename="enc_test_decrypted.txt")

print("--- %s seconds ---" % (time.time() - start_time))
```
![f](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20175742.png)
![f](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20175757.png)
### Section 4: Answer the question
1. Both programs were timed in their execution. The program using the AWS KMS encryption took 1 second to run while the local encryption using PyCryotoDome took 0.55 seconds to run.
![kms](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20174620.png)
![local](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab4/Screenshot%202023-08-24%20175705.png)

# Lab Report 1-4

*Author*: Joo Kai Tay (22489437)

## Lab 1: Introduction & Setup

### Section 1: Creation of AWS Account

1. Navigate to https://489389878001.signin.aws.amazon.com/console and login using the provided username and password, changing the password to a secure one on login.
![AWS login screen](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20210712.jpg)

2. Navigate to the security credentials tab within IAM. Here, the user can view their details such as their ARN and canonical ID.
![Security credentials screen](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211014.jpg)

3. Select the option to create an access key for access to programmatic calls to AWS from the AWS CLI.
![Create Access key](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211030.jpg)

4. Select the option for use with the command line interface (CLI)
![Access key best practices](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211049.jpg)

5. Give the access key a meaningful description tag. This will be useful in the event that you create multiple access keys. The tag should describe the purpose of the key and where it will be used.
![Access key tag](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211109.jpg)

6. Once the key has been successfully created, download the access key and secret access key in the .csv file and store it securely. If the key is not stored securely, any user with access to the key can use it to create and use resources associated with your account. For privacy purposes, the key is censored in this image.
![Successful creation of access key](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211138.jpg)

### Section 2: VM Setup 

The Ubuntu virtual machine was already setup prior to the commencement of this unit. The specifications can be found in the attached image.
![Virtual machine specifications](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211237.jpg)

### Section 3: Software Setup

1. Python 3.10.6 and pip3 were already installed on the machine prior to the commencement of the unit.
![Python installation](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211353.jpg)

2. The AWS CLI was installed and configured using the access key generated as part of step 6 in section 1
![Configuring AWS](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211531.jpg)

3. Boto3 was installed as per the instructions. Boto3 is the AWS SDK for python and it provides a python API for AWS infrastructure services.
4. ![Installing boto3](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211547.jpg)

### Section 4: Exploring and testing the environment 

1. Running the command `aws ec2 describe-regions --output table` is used to retrieve information about the available AWS regions for the EC2 service and display the output in a tabular format. The output of the command is shown below:
![Command output](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211610.jpg)

2. We can retrieve similar information using Python3 by running the following commands. This returns and prints the same data as before. However, this data is not tabulated.
```
python3
>>> import boto3
>>> ec2 = boto3.client('ec2')
>>> response = ec2.describe_regions()
>>> print(response)
```
![python method](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211626.jpg)

3. The following code allows us to tabulate the data and print out only the columns containing the endpoint and RegionName:
```
import boto3

# Get the region data from boto3
ec2 = boto3.client('ec2')
response = ec2.describe_regions()

# Print the header for the columns
print(f"{'Endpoint':<40} {'RegionName'}")

# Print each region's endpoint and region name
for region in response['Regions']:
    print(f"{region['Endpoint']:<40} {region['RegionName']}")
```
The execution of this code (stored in a filed named cits5503_lab1.py) is shown below:
![python file](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab1/Screenshot%202023-08-03%20211644.jpg)

## Lab 2: EC2 and Docker

### Section 1: Creating an EC2 instance using awscli

1. Using the command `aws ec2 create-security-group --group-name <student number>-sg --description "security group for development environment"`, a security group is created. The security group ID is noted for use in future steps. 
![Creating security group](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20164650.jpg)

2. The following command is used to authorise inbound traffic for ssh: `aws ec2 authorize-security-group-ingress --group-name <student number>-sg --protocol tcp --port 22 --cidr 0.0.0.0/0`. Some things to note about this command:
    a. The TCP protocol is specified using `--protocol tcp --port 22` and the default port 22 is selected.
    b. The command `--cidr 0.0.0.0/0` is used as a wildcard. This allows all IP addresses to connect to SSH port that was opened. This is considered a major security risk as it allows anyone on the internet to attempt SSH connections to instances associated with this security group.
![Authorise security group](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20164856.jpg)

3. The command `aws ec2 create-key-pair --key-name <student number>-key --query 'KeyMaterial' --output text > <student number>-key.pem` creates a key pair that will allow a user to ssh to the EC2 instance. The private key is stored in the output file `22489437-key.pem`
![Creating private key](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165022.jpg)

4. The private key file is moved into the SSH directory and its permissions are changed to be read only for the owner of the file, and no permissions for others.
![changing permissions](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165358.jpg)

5. The command ` aws ec2 run-instances --image-id ami-d38a4ab1 --security-group-ids <student number>-sg --count 1 --instance-type t2.micro --key-name <student number>-key --query 'Instances[0].InstanceId' is used to create the EC2 instance. The command returns the instance ID of the created instance.
![creating EC2 instance](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165508.jpg)

6. Using the instance ID from step 5, a tag is added to the instance.
![create tag](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165508.jpg)

7. The public IP address of this instance is determined using the command `aws ec2 describe-instances --instance-ids i-<instance id from above> --query 'Reservations[0].Instances[0].PublicIpAddress'`
![public IP](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165711.jpg)

8. Using the private key file from step 3 and the IP address from step 7, the user is able to SSH into the EC2 instance.
![ssh](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20165859.jpg)

9. The created instance can be viewed from the AWS console
![AWS console](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20170050.jpg)

10. The instance can be terminated from the awscli using the following command: `aws ec2 terminate-instances --instance-ids i-<your instance id>`
![terminating the instance](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-04%20170214.jpg)

## Section 2: Creating an AWS EC2 Instance with Python Boto Script

1. The following code is used to replicate steps 1-7 from section 1:
```
import boto3
import os
import shutil

student_number = "22489437"
region = "ap-southeast-2"

# Initialize the EC2 client
ec2 = boto3.client('ec2', region)

# Create a security group
response = ec2.create_security_group(
    GroupName=f"{student_number}-sg",
    Description="security group for development environment"
)
security_group_id = response['GroupId']

# Authorize inbound SSH traffic for the security group
ec2.authorize_security_group_ingress(
    GroupId=security_group_id,
    IpProtocol="tcp",
    FromPort=22,
    ToPort=22,
    CidrIp="0.0.0.0/0"
)

# Create a key pair and save the private key to a file
response = ec2.create_key_pair(KeyName=f"{student_number}-key")
private_key = response['KeyMaterial']

private_key_file = f"{student_number}-key.pem"
# Allow writing to the private key file
os.chmod(private_key_file, 0o666)
with open(private_key_file, 'w') as key_file:
    key_file.write(private_key)

# Set the correct permissions for the private key file
os.chmod(private_key_file, 0o400)

# Copy the private key file to ~/.ssh directory
ssh_directory = os.path.expanduser("~/.ssh")
if not os.path.exists(ssh_directory):
    os.makedirs(ssh_directory)
shutil.copy(private_key_file, ssh_directory)

# Launch an EC2 instance
response = ec2.run_instances(
    ImageId="ami-d38a4ab1",
    SecurityGroupIds=[security_group_id],
    MinCount=1,
    MaxCount=1,
    InstanceType="t2.micro",
    KeyName=f"{student_number}-key"
)
instance_id = response['Instances'][0]['InstanceId']

# Wait for the instance to be up and running
ec2.get_waiter('instance_running').wait(InstanceIds=[instance_id])

# Describe the instance to get its public IP address
response = ec2.describe_instances(InstanceIds=[instance_id])
public_ip_address = response['Reservations'][0]['Instances'][0]['PublicIpAddress']
print(f"Instance created successfully with Public IP: {public_ip_address}")
```
2. An error was shown when running the script. This was due to the existing key pair that was created in step 1. In order to resolve this error, we head into the AWS console and delete the previously created key pair.
![key pair error](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123521.jpg)
![deleting key pair](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123557.jpg)

3. A second error was shown when running the script. This was due to the existing security group that was created in step 1. In order to resolve this error, we head into the AWS console and delete the previously created security group.
![security group error](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123540.jpg)
![deleting security group](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20123647.jpg)

4. The script ran successfully and returned he public IP of the created EC2 instance:
![public IP](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20125808.jpg)

5. The details of the EC2 instance on the AWS console:
![details](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20125823.jpg)

6. This instance was terminated using the AWS console instead of the AWSCLI, demonstrating the second way to terminate an EC2 instance:
![terminate](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131032.jpg)

### Section 3: Using Docker

1. Docker was installed using the command `sudo apt install docker.io -y`
![installing docker](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131356.jpg)

2. Enabling docker and checking the version installed:
![check docker](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131558.jpg)

3. Creating index.html:
![index](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20131742.jpg)

4. Creating the Dockerfile
![dockerfile](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133452.jpg)

5. Building the docker image from the dockerfile and index.html
![build](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133827.jpg)

6. Running the docker container:
![run](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133907.jpg)

7. Confirming the "Hello World!" output:
![confirm](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20133934.jpg)

8. Viewing what is running:
![view](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20134001.jpg)

9. Terminating the container:
![terminate](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20134036.jpg)
![terminate](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab2/Screenshot%202023-08-07%20134052.jpg)

## Lab 3: Configure S3 Buckets

1. Preparation:
    a. Downloading cloudstorage.py
       ![download](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-13%20152943.png)
   b. Creating rootdir
       ![rootdir](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-13%20153423.png)
   c. Creating subdir
       ![subdir](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-13%20153612.png)
2. Editing cloudstorage.py to take the initialise argument. The following code segment uses the argparse library to allow the argparse library to take 2 arguments `-i` or `-initialise`. Using either of these 2 arguments will set the `bucketFlag` variable to True. If `bucketFlag` is True, we will attempt to create a bucket in S3. We also catch any exceptions such as `BucketAlreadyExists` and `BucketAlreadyOwnedByYou`. 
   ```
       	argParser = argparse.ArgumentParser()
    	argParser.add_argument("-i", "--initialise", action='store_true', help="create bucket on s3")
    	args = argParser.parse_args()
    	bucketFlag = args.initialise
    	# Insert code to create bucket if not there
    	if(bucketFlag):
    		try:
    			response = s3.create_bucket(Bucket=ROOT_S3_DIR, CreateBucketConfiguration=bucket_config)
    			print("Bucket created successfully:", ROOT_S3_DIR)
    			print(response)
    		except s3.exceptions.BucketAlreadyExists:
    			print("Bucket already exists:", ROOT_S3_DIR)
    		except s3.exceptions.BucketAlreadyOwnedByYou:
    			print("Bucket already owned by you:", ROOT_S3_DIR)
    		except Exception as error:
    			print("An error occurred:", error)
    ```
   3. The `upload_file()` method found in `cloudstoraage.py` was modified to add the code required to upload files to S3. The path of the file in S3 is constructed by adding the `ROOT_S3_DIR` which was created in step 2 to the folder path and file name. The file is then uploaded to S3 using the `s3.upload_file()` command.
      ```
          def upload_file(folder_name, file, file_name):
        	# Construct the path for files to be uploaded
        	s3Path = f'{ROOT_S3_DIR}/{folder_name}/{file_name}'
        
        	print("Uploading %s" % file)
        	
        	# Upload the file to S3
        	try:
        		s3.upload_file(file, ROOT_S3_DIR, s3Path)
        		print(f"Uploaded {file_name} to S3")
        	except Exception as e:
        		print(f"Error uploading {file_name}: {e}")
    ```
        ![uploads](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20115517.png)
    4. The following code which is saved in restorefromcloud.py reads the content of the s3 bucket using the command `s3.list_objects` and writes them to the local directory. If the folder structure does not exist in the local directory, the code uses `os.makedirs` to recreate the folder structure from s3 in the local directory.
    ```
        import os
        import boto3
        ROOT_DIR = '.'
        ROOT_S3_DIR = '22489437-cloudstorage'
        REGION = 'ap-southeast-2'
        
        s3 = boto3.client("s3", region_name=REGION)
        bucket_config = {'LocationConstraint': 'ap-southeast-2'}
        
        for key in s3.list_objects(Bucket = ROOT_S3_DIR)['Contents']:
        	if not os.path.exists(os.path.dirname(key['Key'])):
        		os.makedirs(os.path.dirname(key['Key']))
        		
        	print("Downloading %s" % key['Key'])
        	s3.download_file(ROOT_S3_DIR, key['Key'],  key['Key'])
    ```
    ![downloads](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20131746.png)
    5. Setting up DynamoDB locally
        ![createdir](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132006.png)
        ![installjre](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132014.png)
        ![downloaddb](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132048.png)
        ![start](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20132216.png)
    6. Creating a table with the following attributes:
        ```
            CloudFiles = {
                    'userId',
                    'fileName',
                    'path',
                    'lastUpdated',
        	    'owner',
                    'permissions'
                    }
                )
        ```
        ![createtable](https://github.com/Bsubs/CITS5503-Cloud-Computing/blob/main/lab3/Screenshot%202023-08-14%20143839.png)
    
    7. The following code which is saved in storeinfo.py is used to get the file information from s3 and put it into the DynamoDB table created in step 6.
        ```
            import boto3

            ROOT_S3_DIR = '22489437-cloudstorage'  
            REGION = 'ap-southeast-2'  
            TABLE = 'CloudFiles'  
            
            # Initialize AWS resources
            dynamodb = boto3.resource('dynamodb', region_name=REGION, endpoint_url='http://localhost:8000')  
            table = dynamodb.Table(TABLE)  
            s3 = boto3.client('s3', region_name=REGION)  
            
            # List objects in the specified S3 bucket
            response = s3.list_objects(Bucket=ROOT_S3_DIR)
            
            # Extract relevant information from the S3 response
            userId = str(response['Contents'][0]['Owner']['ID'])
            owner = response['Contents'][0]['Owner']['DisplayName']
            permission = s3.get_bucket_acl(Bucket=ROOT_S3_DIR)['Grants'][0]['Permission']
            
            
            # Iterate through each object in the S3 bucket
            for content in response['Contents']:
                # Create an item for DynamoDB with relevant attributes
                item = {
                    'userId': userId,
                    'fileName': content['Key'].split('/')[-1],  # Extract the file name from the full S3 object key
                    'path': content['Key'],
                    'lastUpdated': str(content['LastModified']),
                    'owner': owner,
                    'permissions': permission
                }
                
                print('Putting the following item into DynamoDB table:\n', item, '\n')
                
                # Add the item to the DynamoDB table
                table.put_item(Item=item)
    ```
    


